{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from easydict import EasyDict as edict\n",
    "\n",
    "# modules\n",
    "from dataloader import Cifar10DataLoader\n",
    "from dnn import DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_args = edict({\"batch_size\": 128, \"epochs\": 100})\n",
    "dataloader = Cifar10DataLoader(dataloader_args=dataloader_args)\n",
    "source_train_ds,source_test_ds,target_train_ds,target_test_ds = dataloader.load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = edict({\"units\":[128,64,32,5], \"activations\":[\"relu\",\"relu\",\"relu\",\"softmax\"]})\n",
    "model = DNN(units=model_args.units, activations=model_args.activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "mt_loss_fn = tf.keras.metrics.Mean()\n",
    "test_loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "mte_loss_fn = tf.keras.metrics.Mean()\n",
    "\n",
    "train_metrics = tf.keras.metrics.CategoricalAccuracy()\n",
    "test_metrics = tf.keras.metrics.CategoricalAccuracy()\n",
    "optimizer = tf.keras.optimizers.SGD(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function(experimental_relax_shapes=True, experimental_compile=None)\n",
    "def _train_step(inputs, labels, first_batch=False):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs)\n",
    "        loss = train_loss_fn(labels, predictions)\n",
    "        metrics = tf.reduce_mean(train_metrics(labels, predictions))\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    mt_loss_fn.update_state(loss)\n",
    "    \n",
    "    return loss, metrics\n",
    "\n",
    "def _test_step(inputs, labels):\n",
    "    predictions = model(inputs)\n",
    "    loss = test_loss_fn(labels, predictions)\n",
    "    metrics = tf.reduce_mean(test_metrics(labels, predictions))\n",
    "    mte_loss_fn.update_state(loss)\n",
    "    \n",
    "    return loss, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_buffer = []\n",
    "def copy_weights(variables):\n",
    "    weights = [w.numpy() for w in variables]\n",
    "    model_buffer.append(DNN(units=model_args.units, \n",
    "                            activations=model_args.activations,\n",
    "                            init_value=weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_iter_train = iter(source_train_ds)\n",
    "source_iter_test = iter(source_test_ds)\n",
    "def train_source_models(sample_gap=20):\n",
    "    for e in range(dataloader.source_info.epochs):\n",
    "        mt_loss_fn.reset_states()\n",
    "        train_metrics.reset_states()\n",
    "        mte_loss_fn.reset_states()\n",
    "        test_metrics.reset_states()\n",
    "        for step in range(dataloader.source_info.train_step):\n",
    "            data = source_iter_train.get_next()\n",
    "            train_loss, acc = _train_step(inputs=data[\"inputs\"], labels=data[\"labels\"])\n",
    "            if (e*dataloader.source_info.train_step + step)%sample_gap ==0:\n",
    "                copy_weights(model.trainable_variables)\n",
    "        for step in range(dataloader.source_info.test_step):\n",
    "            data = source_iter_test.get_next()\n",
    "            test_loss, test_acc = _test_step(inputs=data[\"inputs\"], labels=data[\"labels\"])\n",
    "        print(\"Epoch:{}, Train loss: {}, Train acc: {}, Test loss:{}, Test acc:{}\".format(e,\n",
    "                                                                    mt_loss_fn.result().numpy(), \n",
    "                                                                    train_metrics.result().numpy(),\n",
    "                                                                    mte_loss_fn.result().numpy(),\n",
    "                                                                    test_metrics.result().numpy()))\n",
    "# train_source_models(sample_gap=20) \n",
    "# print(len(model_buffer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gmodel_test_step(gmodel, inputs, labels):\n",
    "    predictions = gmodel(inputs)\n",
    "    loss = test_loss_fn(labels, predictions)\n",
    "    metrics = tf.reduce_mean(test_metrics(labels, predictions))\n",
    "    mte_loss_fn.update_state(loss)\n",
    "    return loss, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_iter_test = iter(source_test_ds)\n",
    "def test_models_on_targets():\n",
    "    for idx in range(len(model_buffer)):\n",
    "        mte_loss_fn.reset_states()\n",
    "        test_metrics.reset_states()\n",
    "        for step in range(1):\n",
    "            data = source_iter_test.get_next()\n",
    "            test_loss, test_acc = gmodel_test_step(gmodel=model_buffer[idx], inputs=data[\"inputs\"], labels=data[\"labels\"])\n",
    "        print(\"M_id:{}, Test loss:{}, Test acc:{}\".format(idx,\n",
    "                                                        mte_loss_fn.result().numpy(),\n",
    "                                                        test_metrics.result().numpy()))\n",
    "# test_models_on_targets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def hard_save_gmodels(gmodels, path=\"./models\"):\n",
    "    for idx in range(len(gmodels)):\n",
    "        mpath = os.path.join(path, \"gmodel_{}\".format(idx))\n",
    "        gmodels[idx].save(mpath, overwrite=True, save_format='tf')\n",
    "\n",
    "def load_gmodels_hard(path=\"./models\"):\n",
    "    gmodels = []\n",
    "    gmodel_list = os.listdir(path=path)\n",
    "    for idx in range(len(gmodel_list)):\n",
    "        mpath = os.path.join(path,  \"gmodel_{}\".format(idx))\n",
    "        gmodels.append(tf.keras.models.load_model(mpath))\n",
    "    return gmodels\n",
    "# hard_save_gmodels(gmodels=model_buffer)\n",
    "model_buffer = load_gmodels_hard()\n",
    "test_models_on_targets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _gmodel_train_step(gmodels, inputs, labels, gopt):\n",
    "    ggrads = []\n",
    "    for m in gmodels:\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = m(inputs)\n",
    "            loss = train_loss_fn(labels, predictions)\n",
    "            metrics = tf.reduce_mean(train_metrics(labels, predictions))\n",
    "            grad = tape.gradient(loss, m.trainable_variables)\n",
    "        ggrads.append(grad)\n",
    "        mt_loss_fn.update_state(loss)\n",
    "        \n",
    "    mgrad = []\n",
    "    for i in range(len(ggrads[0])):\n",
    "        w = []\n",
    "        for j in range(len(gmodels)):\n",
    "            w.append(ggrads[j][i])\n",
    "        mgrad.append(tf.reduce_sum(w, axis=0))\n",
    "        \n",
    "    for m in gmodels:\n",
    "        gopt.apply_gradients(zip(mgrad, m.trainable_variables))\n",
    "    \n",
    "    return loss, metrics\n",
    "\n",
    "max_metrics = tf.keras.metrics.Mean()\n",
    "def _gmodel_test_step(gmodels, inputs, labels):\n",
    "    losses = []\n",
    "    m_metrics = []\n",
    "    for m in gmodels:\n",
    "        predictions = m(inputs)\n",
    "        loss = test_loss_fn(labels, predictions)\n",
    "        losses.append(loss)\n",
    "        metrics = tf.reduce_mean(test_metrics(labels, predictions))\n",
    "        m_metrics.append(metrics)\n",
    "        mte_loss_fn.update_state(loss)\n",
    "    max_metrics.update_state(max(m_metrics))\n",
    "    return losses, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained():\n",
    "    target_iter_train = iter(target_train_ds)\n",
    "    target_iter_test = iter(target_test_ds)\n",
    "    data = target_iter_train.get_next()\n",
    "    gopt = tf.keras.optimizers.SGD(0.01)\n",
    "    model_idx = [-1]\n",
    "    gmodels = [model_buffer[idx] for idx in model_idx]\n",
    "    import random\n",
    "\n",
    "    for e in range(100):\n",
    "        mt_loss_fn.reset_states()\n",
    "        train_metrics.reset_states()\n",
    "        mte_loss_fn.reset_states()\n",
    "        test_metrics.reset_states()\n",
    "        max_metrics.reset_states()\n",
    "        for step in range(1):\n",
    "            train_loss, train_acc = _gmodel_train_step(gmodels=gmodels, \n",
    "                                                    inputs=data[\"inputs\"], \n",
    "                                                    labels=data[\"labels\"],\n",
    "                                                    gopt=gopt)\n",
    "        \n",
    "        for step in range(100):\n",
    "            te_data = target_iter_test.get_next()\n",
    "            test_loss, test_acc = _gmodel_test_step(gmodels=gmodels, \n",
    "                                                    inputs=te_data[\"inputs\"], \n",
    "                                                    labels=te_data[\"labels\"]\n",
    "                                                    )\n",
    "            \n",
    "        # for step in range()\n",
    "        print(\"Epoch:{}, Train loss: {}, Train acc: {}, Test loss:{}, Test acc:{}, Max acc:{}\".format(e,\n",
    "                                                                mt_loss_fn.result().numpy(), \n",
    "                                                                train_metrics.result().numpy(),\n",
    "                                                                mte_loss_fn.result().numpy(),\n",
    "                                                                test_metrics.result().numpy(), max_metrics.result.numpy()))\n",
    "# pretrained()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_random_init():\n",
    "    target_iter_train = iter(target_train_ds)\n",
    "    target_iter_test = iter(target_test_ds)\n",
    "    data = target_iter_train.get_next()\n",
    "    gopt = tf.keras.optimizers.SGD(0.01)\n",
    "    model_idx = [1]\n",
    "    gmodels = [model_buffer[idx] for idx in model_idx]\n",
    "    import random\n",
    "\n",
    "    for e in range(100):\n",
    "        mt_loss_fn.reset_states()\n",
    "        train_metrics.reset_states()\n",
    "        mte_loss_fn.reset_states()\n",
    "        test_metrics.reset_states()\n",
    "        max_metrics.reset_states()\n",
    "        for step in range(1):\n",
    "            train_loss, train_acc = _gmodel_train_step(gmodels=gmodels, \n",
    "                                                    inputs=data[\"inputs\"], \n",
    "                                                    labels=data[\"labels\"],\n",
    "                                                    gopt=gopt)\n",
    "        \n",
    "        for step in range(100):\n",
    "            te_data = target_iter_test.get_next()\n",
    "            test_loss, test_acc = _gmodel_test_step(gmodels=gmodels, \n",
    "                                                    inputs=te_data[\"inputs\"], \n",
    "                                                    labels=te_data[\"labels\"]\n",
    "                                                    )\n",
    "            \n",
    "        # for step in range()\n",
    "        print(\"Epoch:{}, Train loss: {}, Train acc: {}, Test loss:{}, Test acc:{}\".format(e,\n",
    "                                                                mt_loss_fn.result().numpy(), \n",
    "                                                                train_metrics.result().numpy(),\n",
    "                                                                mte_loss_fn.result().numpy(),\n",
    "                                                                test_metrics.result().numpy()))\n",
    "# from_random_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L2L(n=32):\n",
    "    target_iter_train = iter(target_train_ds)\n",
    "    target_iter_test = iter(target_test_ds)\n",
    "    data = target_iter_train.get_next()\n",
    "    \n",
    "    gopt = optimizer\n",
    "    import random\n",
    "\n",
    "    for e in range(100):\n",
    "        mt_loss_fn.reset_states()\n",
    "        train_metrics.reset_states()\n",
    "        mte_loss_fn.reset_states()\n",
    "        test_metrics.reset_states()\n",
    "        max_metrics.reset_states()\n",
    "        for step in range(1):\n",
    "            model_idx = random.sample(range(len(model_buffer)), n)\n",
    "            # model_idx = []\n",
    "            gmodels = [model_buffer[idx] for idx in model_idx]\n",
    "            train_loss, train_acc = _gmodel_train_step(gmodels=gmodels, \n",
    "                                                    inputs=data[\"inputs\"], \n",
    "                                                    labels=data[\"labels\"],\n",
    "                                                    gopt=gopt)\n",
    "        # for step in range()\n",
    "        for step in range(100):\n",
    "            model_idx = random.sample(range(len(model_buffer)), n)\n",
    "            # model_idx = [-n, -n+386,-n+128, -n+256, -n+96, -n+4,-n+8,-n+16,-n+32,-n+64,-n+2,-n+1]\n",
    "            gmodels = [model_buffer[idx] for idx in model_idx]\n",
    "            te_data = target_iter_test.get_next()\n",
    "            test_loss, test_acc = _gmodel_test_step(gmodels=gmodels, \n",
    "                                                    inputs=te_data[\"inputs\"], \n",
    "                                                    labels=te_data[\"labels\"]\n",
    "                                                    )\n",
    "        print(\"Epoch:{}, Train loss: {}, Train acc: {}, Test loss:{}, Test acc:{}, Max acc:{}\".format(e,\n",
    "                                                        mt_loss_fn.result().numpy(), \n",
    "                                                        train_metrics.result().numpy(),\n",
    "                                                        mte_loss_fn.result().numpy(),\n",
    "                                                        test_metrics.result().numpy(), max_metrics.result().numpy()))\n",
    "L2L()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Jun  1 2022, 06:34:44) \n[Clang 12.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "14ddbbabdf1cc89aed24e001be3922f4034073f682acd1338a644a14376bd924"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
